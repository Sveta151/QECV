{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84bb7ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66102dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=\"API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db2534e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../final/q3_with_summaries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbf1fa31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['claim_id', 'claim', 'generated_question_1', 'document_url_question_1',\n",
       "       'summary_1', 'label_1', 'generated_question_2', 'summary_2', 'label_2',\n",
       "       'claim_date', 'speaker', 'reporting_source', 'document_url_question_2',\n",
       "       'judgement', 'verification_status', 'generated_question_3',\n",
       "       'document_weight_question_3', 'document_rank_question_3',\n",
       "       'document_url_question_3', 'document_question_3', 'summary_3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e71ae7",
   "metadata": {},
   "source": [
    "### finding maximum siffix that we are going to use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6312922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Assume merged_df is your DataFrame\n",
    "df_columns = df.columns\n",
    "\n",
    "def get_max_suffix(columns):\n",
    "    max_suffix = 0\n",
    "    for column in columns:\n",
    "        match = re.search(r'(\\d+)$', column)\n",
    "        if match:\n",
    "            suffix = int(match.group(1))\n",
    "            if suffix > max_suffix:\n",
    "                max_suffix = suffix\n",
    "    return max_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe811a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SUFFIX = get_max_suffix(df_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b08c37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum suffix number is: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"The maximum suffix number is:\", MAX_SUFFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19141397",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[f'label_{MAX_SUFFIX}'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0d5790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## some changes for this df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87f597e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.rename(columns={'summary': 'summary_2'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "344c5185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b7beda4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "733f0dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['claim_id'] != 'claim_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "570477fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_1\n",
       "Refuted    10\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label_1'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60b4fa9",
   "metadata": {},
   "source": [
    "### Our prompts for further evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1086ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECK_PROMPT = \"\"\" Given the provided claim, question, and summary of the text, determine the most appropriate label for the relationship between the claim and the summary, assuming the summary serves as an answer to the question. The possible labels are:\n",
    "\n",
    "1. **Refuted**: The summary clearly contradicts the claim.\n",
    "2. **Supported**: The summary clearly supports the claim.\n",
    "3. **Not Enough Evidence**: The summary does not provide sufficient evidence to either support or refute the claim. If there are no information in the text it means its Not Enough Evidence.\n",
    "\n",
    "Claim is : [[CLAIM]]\n",
    "Question is : [[QUESTION]]\n",
    "Summary is: [[SUMMARY]]\n",
    "Based on this information, what label can you give?\n",
    "Just provide final label.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb42ad11",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOOSING_BEST_PROMPT = \"\"\"\n",
    "Based on the provided claim, question, and summaries, determine which summary provides the most relevant information related to the claim and question.\n",
    "\n",
    "Claim: [[CLAIM]]\n",
    "Question: [[QUESTION]]\n",
    "\n",
    "Summaries:\n",
    "[[SUMMARIES]]\n",
    "\n",
    "Please choose the summary that best addresses the claim and question by providing its index.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba3f33c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "63ae9881",
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGEMENT_PROMPT = \"\"\"\n",
    "Based on the provided claim, questions and summaries, answer the questions in a single answer.\n",
    "Claim: [[CLAIM]]\n",
    "Question 1: [[QUESTION_1]]\n",
    "Summary 1: [[SUMMARY_1]]\n",
    "Question 2: [[QUESTION_2]]\n",
    "Summary 2: [[SUMMARY_2]]\n",
    "Question 3: [[QUESTION_3]]\n",
    "Summary 3: [[SUMMARY_3]]\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "318181e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_DEMO_STOP = '''Claim = Superdrag and Collective Soul are both rock bands.\n",
    "To validate the above claim, we have asked the following questions: \n",
    "Question 1 = Is Superdrag a rock band?\n",
    "Answer 1 = Yes\n",
    "Can we know whether the claim is true or false now?\n",
    "Prediction = No, we cannot know. \n",
    "\n",
    "Claim = Superdrag and Collective Soul are both rock bands.\n",
    "To validate the above claim, we have asked the following questions: \n",
    "Question 1 = Is Superdrag a rock band?\n",
    "Answer 1 = Yes\n",
    "Question 2 = Is Collective Soul a rock band?\n",
    "Answer 2 = Yes\n",
    "Can we know whether the claim is true or false now?\n",
    "Prediction = Yes, we can know.\n",
    "\n",
    "Claim = Jimmy Garcia lost by unanimous decision to a professional boxer that challenged for the WBO lightweight title in 1995. \n",
    "To validate the above claim, we have asked the following questions:\n",
    "Question 1 = Who is the professional boxer that challenged for the WBO lightweight title in 1995? \n",
    "Answer 1 = Orzubek Nazarov\n",
    "Can we know whether the claim is true or false now?\n",
    "Prediction = No, we cannot know.\n",
    "\n",
    "Claim = Jimmy Garcia lost by unanimous decision to a professional boxer that challenged for the WBO lightweight title in 1995. \n",
    "To validate the above claim, we have asked the following questions:\n",
    "Question 1 = Who is the professional boxer that challenged for the WBO lightweight title in 1995? \n",
    "Answer 1 = Orzubek Nazarov\n",
    "Question 2 = Did Jimmy Garcia lose by unanimous decision to Orzubek Nazarov?\n",
    "Can we know whether the claim is true or false now?\n",
    "Prediction = Yes, we can know.\n",
    "\n",
    "Claim = The Swan of Catania was taught by the Italian composer Giovanni Furno.\n",
    "To validate the above claim, we have asked the following questions: \n",
    "Question 1 = What is the nationality of Giovanni Furno?\n",
    "Answer 1 = Italian\n",
    "Can we know whether the claim is true or false now?\n",
    "Prediction = No, we cannot know.\n",
    "\n",
    "Claim = Lars Onsager won the Nobel prize when he was 30 years old.\n",
    "To validate the above claim, we have asked the following questions:  \n",
    "Question 1 = When Lars Onsager won the Nobel prize?\n",
    "Answer 1 = 1968\n",
    "Can we know whether the claim is true or false now?\n",
    "Prediction = No, we cannot know.\n",
    "\n",
    "Claim = Smith worked on the series The Handmaid's Tale that is based on a novel by Margaret Atwood. \n",
    "To validate the above claim, we have asked the following questions:\n",
    "Question 1 = Which novel The Handmaid's Tale is based on?\n",
    "Answer 1 = Margaret Atwood\n",
    "Can we know whether the claim is true or false now?\n",
    "Prediction = No, we cannot know.\n",
    "\n",
    "Claim = Smith worked on the series The Handmaid's Tale that is based on a novel by Margaret Atwood. \n",
    "To validate the above claim, we have asked the following questions:\n",
    "Question 1 = Which novel The Handmaid's Tale is based on?\n",
    "Answer 1 = Margaret Atwood\n",
    "Question 2 = Did Smith work on the series The Handmaid's Tale?\n",
    "Answer 2 = Yes\n",
    "Can we know whether the claim is true or false now?\n",
    "Prediction = Yes, we can know.\n",
    "\n",
    "Claim = The first season of the series The Handmaid's Tale was released in 2017.\n",
    "To validate the above claim, we have asked the following questions:\n",
    "Question 1 = When was the first season of the series The Handmaid's Tale released?\n",
    "Answer 1 = 2017\n",
    "Can we know whether the claim is true or false now?\n",
    "Prediction = Yes, we can know.\n",
    "\n",
    "Claim = [[CLAIM]]\n",
    "To validate the above claim, we have asked the following questions:\n",
    "[[QA_CONTEXTS]]\n",
    "Can we know whether the claim is true or false now?\n",
    "Prediction = '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "886ef6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_DEMO_SUBSEQUENT = '''Task: to verify a claim, we need to ask a series of simple questions. Here the task is given a claim and previous questions generate the following question to ask. \n",
    "This question should be:\n",
    "\n",
    "- Simple with a single subject-verb-object structure.\n",
    "- Specific and directly related to the key aspect of the claim that needs validation.\n",
    "\n",
    "Claim = Superdrag and Collective Soul are both rock bands.\n",
    "To validate the above claim, we need to ask the following simple questions sequentially: \n",
    "Question 1 = Is Superdrag a rock band?\n",
    "Answer 1 = Yes\n",
    "Question 2 = Is Collective Soul a rock band?\n",
    "\n",
    "Claim = Jimmy Garcia lost by unanimous decision to a professional boxer that challenged for the WBO lightweight title in 1995. \n",
    "To validate the above claim, we need to ask the following simple questions sequentially: \n",
    "Question 1 = Who is the professional boxer that challenged for the WBO lightweight title in 1995? \n",
    "Answer 1 = Orzubek Nazarov\n",
    "Question 2 = Did Jimmy Garcia lose by unanimous decision to Orzubek Nazarov?\n",
    "\n",
    "Claim = The Swan of Catania was taught by the Italian composer Giovanni Furno.\n",
    "To validate the above claim, we need to ask the following simple questions sequentially: \n",
    "Question 1 = What is the nationality of Giovanni Furno?\n",
    "Answer 1 = Italian\n",
    "Question 2 = Who was taught by Giovanni Furno?\n",
    "\n",
    "Claim = Smith worked on the series The Handmaid's Tale that is based on a novel by Margaret Atwood.\n",
    "To validate the above claim, we need to ask the following simple questions sequentially:\n",
    "Question 1 = Which novel The Handmaid's Tale is based on?\n",
    "Answer 1 = Margaret Atwood\n",
    "Question 2 = Who worked on the series The Handmaid's Tale?\n",
    "\n",
    "Claim = The Potomac River runs along the neighborhood where Ashley Estates Kavanaugh's wedding was held.\n",
    "To validate the above claim, we need to ask the following simple questions sequentially:\n",
    "Question 1 = Where was Ashley Estates Kavanaugh's wedding held?\n",
    "Answer 1 = Christ Church in Georgetown\n",
    "Question 2 = Which river runs along the Christ Church in Georgetown?\n",
    "\n",
    "Claim = Ulrich Walter's employer is headquartered in Cologne.\n",
    "To validate the above claim, we need to ask the following simple questions sequentially:\n",
    "Question 1 = Who is Ulrich Walter's employer?\n",
    "Answer 1 = University of Cologne\n",
    "Question 2 = Where is the University of Cologne headquartered?\n",
    "\n",
    "Claim = Lars Onsager won the Nobel prize when he was 30 years old.\n",
    "To validate the above claim, we need to ask the following simple questions sequentially: \n",
    "Question 1 = When Lars Onsager won the Nobel prize?\n",
    "Answer 1 = 1968\n",
    "Question 2 = When was Lars Onsager born?\n",
    "\n",
    "Claim = [[CLAIM]]\n",
    "To validate the above claim, we need to ask the following simple questions sequentially: \n",
    "[[QA_CONTEXTS]]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a46d5b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL=\"gpt-4o\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71266268",
   "metadata": {},
   "source": [
    "## Label generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00a54fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating labels for each summary\n",
    "# keeping it all in one row after each summaries \n",
    "\n",
    "\n",
    "def check_label(claim, question, summary):\n",
    "    PR_template = CHECK_PROMPT\n",
    "    example_input = PR_template.replace('[[CLAIM]]', claim.strip())\n",
    "    example_input = example_input.replace('[[QUESTION]]', question.strip())\n",
    "    example_input = example_input.replace('[[SUMMARY]]', summary.strip())\n",
    "    completion = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert annotator who assists in determining the relationship between a claim and a summary in the context of a given question. Your task is to label the summary as either Refuted, Supported, or Not Enough Evidence based on how it answers the question in relation to the claim. Just provide final label.\"},\n",
    "            {\"role\": \"user\", \"content\": example_input}\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    claim = row['claim']\n",
    "    question = row[f'generated_question_{MAX_SUFFIX}']\n",
    "    summary = row[f'summary_{MAX_SUFFIX}']\n",
    "    label_column = f'label_{MAX_SUFFIX}'\n",
    "    label = check_label(claim, question, summary)\n",
    "    df.at[index, label_column] = label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1afbf67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_3\n",
       "Supported    10\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[f'label_{MAX_SUFFIX}'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be4cccc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim_id</th>\n",
       "      <th>claim</th>\n",
       "      <th>generated_question_1</th>\n",
       "      <th>document_url_question_1</th>\n",
       "      <th>summary_1</th>\n",
       "      <th>label_1</th>\n",
       "      <th>generated_question_2</th>\n",
       "      <th>summary_2</th>\n",
       "      <th>label_2</th>\n",
       "      <th>claim_date</th>\n",
       "      <th>...</th>\n",
       "      <th>document_url_question_2</th>\n",
       "      <th>judgement</th>\n",
       "      <th>verification_status</th>\n",
       "      <th>generated_question_3</th>\n",
       "      <th>document_weight_question_3</th>\n",
       "      <th>document_rank_question_3</th>\n",
       "      <th>document_url_question_3</th>\n",
       "      <th>document_question_3</th>\n",
       "      <th>summary_3</th>\n",
       "      <th>label_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Due to Imran Khan's criticism of Macron's comm...</td>\n",
       "      <td>Did French authorities cancel the visas of 183...</td>\n",
       "      <td>https://tribune.com.pk/story/1119830/diplomati...</td>\n",
       "      <td>The text contradicts the question. There is no...</td>\n",
       "      <td>Refuted</td>\n",
       "      <td>Did French authorities deport 118 Pakistani ci...</td>\n",
       "      <td>The summarized information contradicts the que...</td>\n",
       "      <td>Refuted</td>\n",
       "      <td>31-10-2020</td>\n",
       "      <td>...</td>\n",
       "      <td>The summarized information contradicts the que...</td>\n",
       "      <td>No, there is no mention of French authorities ...</td>\n",
       "      <td>Yes, we can know.</td>\n",
       "      <td>Did Imran Khan criticize Macron's comments on ...</td>\n",
       "      <td>weighted</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.dw.com/en/pakistans-imran-khan-sla...</td>\n",
       "      <td>Pakistan's Khan slams Macron's views on IslamO...</td>\n",
       "      <td>Yes, Imran Khan criticized Macron's comments o...</td>\n",
       "      <td>Supported</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   claim_id                                              claim  \\\n",
       "0         2  Due to Imran Khan's criticism of Macron's comm...   \n",
       "\n",
       "                                generated_question_1  \\\n",
       "0  Did French authorities cancel the visas of 183...   \n",
       "\n",
       "                             document_url_question_1  \\\n",
       "0  https://tribune.com.pk/story/1119830/diplomati...   \n",
       "\n",
       "                                           summary_1  label_1  \\\n",
       "0  The text contradicts the question. There is no...  Refuted   \n",
       "\n",
       "                                generated_question_2  \\\n",
       "0  Did French authorities deport 118 Pakistani ci...   \n",
       "\n",
       "                                           summary_2  label_2  claim_date  \\\n",
       "0  The summarized information contradicts the que...  Refuted  31-10-2020   \n",
       "\n",
       "   ...                            document_url_question_2  \\\n",
       "0  ...  The summarized information contradicts the que...   \n",
       "\n",
       "                                           judgement verification_status  \\\n",
       "0  No, there is no mention of French authorities ...   Yes, we can know.   \n",
       "\n",
       "                                generated_question_3  \\\n",
       "0  Did Imran Khan criticize Macron's comments on ...   \n",
       "\n",
       "  document_weight_question_3 document_rank_question_3  \\\n",
       "0                   weighted                        1   \n",
       "\n",
       "                             document_url_question_3  \\\n",
       "0  https://www.dw.com/en/pakistans-imran-khan-sla...   \n",
       "\n",
       "                                 document_question_3  \\\n",
       "0  Pakistan's Khan slams Macron's views on IslamO...   \n",
       "\n",
       "                                           summary_3    label_3  \n",
       "0  Yes, Imran Khan criticized Macron's comments o...  Supported  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5bb340aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['claim_id', 'claim', 'claim_date', 'speaker', 'reporting_source',\n",
      "       'generated_question_1', 'document_url_question_1', 'summary_1',\n",
      "       'label_1', 'generated_question_2', 'document_url_question_2',\n",
      "       'summary_2', 'label_2', 'generated_question_3', 'supported_summaries',\n",
      "       'refuted_summaries', 'not_enough_evidence_summaries', 'supported_urls',\n",
      "       'refuted_urls', 'not_enough_evidence_urls'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "unique_pairs = df[['claim_id', 'claim', 'claim_date', 'speaker', 'reporting_source', 'generated_question_1', 'document_url_question_1', 'summary_1', 'label_1', 'generated_question_2', 'document_url_question_2', 'summary_2', 'label_2', 'generated_question_3']].drop_duplicates()\n",
    "\n",
    "unique_pairs['supported_summaries'] = None\n",
    "unique_pairs['refuted_summaries'] = None\n",
    "unique_pairs['not_enough_evidence_summaries'] = None\n",
    "unique_pairs['supported_urls'] = None\n",
    "unique_pairs['refuted_urls'] = None\n",
    "unique_pairs['not_enough_evidence_urls'] = None\n",
    "\n",
    "for idx, unique_pair in unique_pairs.iterrows():\n",
    "    filtered_rows = df[\n",
    "        (df['claim_id'] == unique_pair['claim_id']) &\n",
    "        (df['claim'] == unique_pair['claim']) &\n",
    "        (df['generated_question_1'] == unique_pair['generated_question_1']) &\n",
    "        (df['document_url_question_1'] == unique_pair['document_url_question_1']) &\n",
    "        (df['summary_1'] == unique_pair['summary_1']) &\n",
    "        (df['label_1'] == unique_pair['label_1']) &\n",
    "        (df['generated_question_2'] == unique_pair['generated_question_2']) &\n",
    "        (df['document_url_question_2'] == unique_pair['document_url_question_2']) &\n",
    "        (df['summary_2'] == unique_pair['summary_2']) &\n",
    "        (df['label_2'] == unique_pair['label_2']) &\n",
    "        (df['generated_question_3'] == unique_pair['generated_question_3'])\n",
    "    ]\n",
    "    # print(f'Rows for unique pair {idx}:')\n",
    "    label_supported = filtered_rows[filtered_rows[\"label_3\"] == \"Supported\"][\"summary_3\"]\n",
    "    label_refuted = filtered_rows[filtered_rows[\"label_3\"] == \"Refuted\"][\"summary_3\"]\n",
    "    label_nee = filtered_rows[filtered_rows[\"label_3\"] == \"Not Enough Evidence\"][\"summary_3\"]\n",
    "\n",
    "    supported_url = filtered_rows[filtered_rows[\"label_3\"] == \"Supported\"][\"document_url_question_3\"]\n",
    "    refuted_url = filtered_rows[filtered_rows[\"label_3\"] == \"Refuted\"][\"document_url_question_3\"]\n",
    "    nee_url = filtered_rows[filtered_rows[\"label_3\"] == \"Not Enough Evidence\"][\"document_url_question_3\"]\n",
    "\n",
    "    unique_pairs = unique_pairs.copy()  # Make a copy to avoid SettingWithCopyWarning\n",
    "    unique_pairs.at[idx, 'supported_summaries'] = list(label_supported)\n",
    "    unique_pairs.at[idx, 'refuted_summaries'] = list(label_refuted)\n",
    "    unique_pairs.at[idx, 'not_enough_evidence_summaries'] = list(label_nee)\n",
    "    unique_pairs.at[idx, 'supported_urls'] = list(label_supported)\n",
    "    unique_pairs.at[idx, 'refuted_urls'] = list(label_refuted)\n",
    "    unique_pairs.at[idx, 'not_enough_evidence_urls'] = list(label_nee)\n",
    "print(unique_pairs.columns)\n",
    "processed_df = unique_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b4a750d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim_id</th>\n",
       "      <th>claim</th>\n",
       "      <th>claim_date</th>\n",
       "      <th>speaker</th>\n",
       "      <th>reporting_source</th>\n",
       "      <th>generated_question_1</th>\n",
       "      <th>document_url_question_1</th>\n",
       "      <th>summary_1</th>\n",
       "      <th>label_1</th>\n",
       "      <th>generated_question_2</th>\n",
       "      <th>document_url_question_2</th>\n",
       "      <th>summary_2</th>\n",
       "      <th>label_2</th>\n",
       "      <th>generated_question_3</th>\n",
       "      <th>supported_summaries</th>\n",
       "      <th>refuted_summaries</th>\n",
       "      <th>not_enough_evidence_summaries</th>\n",
       "      <th>supported_urls</th>\n",
       "      <th>refuted_urls</th>\n",
       "      <th>not_enough_evidence_urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Due to Imran Khan's criticism of Macron's comm...</td>\n",
       "      <td>31-10-2020</td>\n",
       "      <td>Consulate General Of Pakistan France</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>Did French authorities cancel the visas of 183...</td>\n",
       "      <td>https://tribune.com.pk/story/1119830/diplomati...</td>\n",
       "      <td>The text contradicts the question. There is no...</td>\n",
       "      <td>Refuted</td>\n",
       "      <td>Did French authorities deport 118 Pakistani ci...</td>\n",
       "      <td>The summarized information contradicts the que...</td>\n",
       "      <td>The summarized information contradicts the que...</td>\n",
       "      <td>Refuted</td>\n",
       "      <td>Did Imran Khan criticize Macron's comments on ...</td>\n",
       "      <td>[Yes, Imran Khan criticized Macron's comments ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Yes, Imran Khan criticized Macron's comments ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   claim_id                                              claim  claim_date  \\\n",
       "0         2  Due to Imran Khan's criticism of Macron's comm...  31-10-2020   \n",
       "\n",
       "                                speaker reporting_source  \\\n",
       "0  Consulate General Of Pakistan France          Twitter   \n",
       "\n",
       "                                generated_question_1  \\\n",
       "0  Did French authorities cancel the visas of 183...   \n",
       "\n",
       "                             document_url_question_1  \\\n",
       "0  https://tribune.com.pk/story/1119830/diplomati...   \n",
       "\n",
       "                                           summary_1  label_1  \\\n",
       "0  The text contradicts the question. There is no...  Refuted   \n",
       "\n",
       "                                generated_question_2  \\\n",
       "0  Did French authorities deport 118 Pakistani ci...   \n",
       "\n",
       "                             document_url_question_2  \\\n",
       "0  The summarized information contradicts the que...   \n",
       "\n",
       "                                           summary_2  label_2  \\\n",
       "0  The summarized information contradicts the que...  Refuted   \n",
       "\n",
       "                                generated_question_3  \\\n",
       "0  Did Imran Khan criticize Macron's comments on ...   \n",
       "\n",
       "                                 supported_summaries refuted_summaries  \\\n",
       "0  [Yes, Imran Khan criticized Macron's comments ...                []   \n",
       "\n",
       "  not_enough_evidence_summaries  \\\n",
       "0                            []   \n",
       "\n",
       "                                      supported_urls refuted_urls  \\\n",
       "0  [Yes, Imran Khan criticized Macron's comments ...           []   \n",
       "\n",
       "  not_enough_evidence_urls  \n",
       "0                       []  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_pairs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f77ea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding best summary and saving question and answer pair\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Imitate GPT call to always return the first index\n",
    "def choose_best_summary(claim, question, summaries):\n",
    "    if not summaries:\n",
    "        return None, None\n",
    "\n",
    "    summaries_text = \"\\n\".join([f\"[[{i}]] - {summary}\" for i, summary in enumerate(summaries)])\n",
    "    prompt = CHOOSING_BEST_PROMPT.replace(\"[[CLAIM]]\", claim).replace(\"[[QUESTION]]\", question).replace(\"[[SUMMARIES]]\", summaries_text)\n",
    "    \n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert annotator who assists in determining best and most informative summary based on the provided claim, question and text.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    content = completion.choices[0].message.content.strip()\n",
    "    \n",
    "    # Extract the index using regex\n",
    "    match = re.search(r'\\[\\[(\\d+)\\]\\]', content)\n",
    "    if not match:\n",
    "        match = re.search(r'(\\d+)', content)\n",
    "    \n",
    "    if match:\n",
    "        best_summary_index = int(match.group(1))\n",
    "        if best_summary_index < len(summaries):\n",
    "            return summaries[best_summary_index], best_summary_index\n",
    "        else:\n",
    "#             raise ValueError(f\"Extracted index {best_summary_index} is out of range for summaries: {content}\")\n",
    "            return summaries[0],0\n",
    "    else:\n",
    "#         raise ValueError(f\"Invalid index format in GPT response: {content}\")\n",
    "        return summaries[0],0\n",
    "\n",
    "\n",
    "# Function to apply the filtering logic and choose the best summary\n",
    "def process_row(row):\n",
    "    supported_summaries = row['supported_summaries'] if isinstance(row['supported_summaries'], list) else eval(row['supported_summaries'])\n",
    "    supported_urls = row['supported_urls'] if isinstance(row['supported_urls'], list) else eval(row['supported_urls'])\n",
    "    refuted_summaries = row['refuted_summaries'] if isinstance(row['refuted_summaries'], list) else eval(row['refuted_summaries'])\n",
    "    refuted_urls = row['refuted_urls'] if isinstance(row['refuted_urls'], list) else eval(row['refuted_urls'])\n",
    "    not_enough_evidence_summaries = row['not_enough_evidence_summaries'] if isinstance(row['not_enough_evidence_summaries'], list) else eval(row['not_enough_evidence_summaries'])\n",
    "    not_enough_evidence_urls = row['not_enough_evidence_urls'] if isinstance(row['not_enough_evidence_urls'], list) else eval(row['not_enough_evidence_urls'])\n",
    "    \n",
    "    claim_id = row['claim_id']\n",
    "    claim = row['claim']\n",
    "    question = row['generated_question_2']\n",
    "    claim_date = row['claim_date']\n",
    "    speaker = row['speaker']\n",
    "    reporting_source = row['reporting_source']\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    if supported_summaries:\n",
    "        if len(supported_summaries) == 1:\n",
    "            best_supported_summary = supported_summaries[0]\n",
    "            best_supported_url = supported_urls[0]\n",
    "        else:\n",
    "            best_supported_summary, best_supported_index = choose_best_summary(claim, question, supported_summaries)\n",
    "            best_supported_url = supported_urls[best_supported_index]\n",
    "        results.append({\n",
    "            'claim_id': claim_id,\n",
    "            'claim': claim,\n",
    "            'claim_date': claim_date,\n",
    "            'speaker': speaker,\n",
    "            'reporting_source': reporting_source,\n",
    "            'generated_question_1': row[\"generated_question_1\"], \n",
    "            'document_url_question_1': row[\"document_url_question_1\"], \n",
    "            'summary_1': row[\"summary_1\"], \n",
    "            'label_1': row[\"label_1\"],\n",
    "            'generated_question_2': row[\"generated_question_2\"], \n",
    "            'document_url_question_2': row[\"document_url_question_2\"], \n",
    "            'summary_2': row[\"summary_2\"], \n",
    "            'label_2': row[\"label_2\"],\n",
    "            'generated_question_3': question,\n",
    "            'document_url_question_3': best_supported_url, \n",
    "            'summary_3': best_supported_summary,\n",
    "            'label_3': 'Supported'\n",
    "        })\n",
    "        \n",
    "    if refuted_summaries:\n",
    "        if len(refuted_summaries) == 1:\n",
    "            best_refuted_summary = refuted_summaries[0]\n",
    "            best_refuted_url = refuted_urls[0]\n",
    "        else:\n",
    "            best_refuted_summary, best_refuted_index = choose_best_summary(claim, question, refuted_summaries)\n",
    "            best_refuted_url = refuted_urls[best_refuted_index]\n",
    "        results.append({\n",
    "            'claim_id': claim_id,\n",
    "            'claim': claim,\n",
    "            'claim_date': claim_date,\n",
    "            'speaker': speaker,\n",
    "            'reporting_source': reporting_source,\n",
    "            'generated_question_1': row[\"generated_question_1\"], \n",
    "            'document_url_question_1': row[\"document_url_question_1\"], \n",
    "            'summary_1': row[\"summary_1\"], \n",
    "            'label_1': row[\"label_1\"],\n",
    "            'generated_question_2': row[\"generated_question_2\"], \n",
    "            'document_url_question_2': row[\"document_url_question_2\"], \n",
    "            'summary_2': row[\"summary_2\"], \n",
    "            'label_2': row[\"label_2\"],\n",
    "            'generated_question_3': question,\n",
    "            'document_url_question_3': best_refuted_url, \n",
    "            'summary_3': best_refuted_summary,\n",
    "            'label_3': 'Refuted'\n",
    "        })\n",
    "        \n",
    "    if not results and not_enough_evidence_summaries:\n",
    "        if len(not_enough_evidence_summaries) == 1:\n",
    "            best_not_enough_evidence_summary = not_enough_evidence_summaries[0]\n",
    "            best_not_enough_evidence_url = not_enough_evidence_urls[0]\n",
    "        else:\n",
    "            best_not_enough_evidence_summary, best_not_enough_evidence_index = choose_best_summary(claim, question, not_enough_evidence_summaries)\n",
    "            best_not_enough_evidence_url = not_enough_evidence_urls[best_not_enough_evidence_index]\n",
    "        results.append({\n",
    "\n",
    "            'claim_id': claim_id,\n",
    "            'claim': claim,\n",
    "            'claim_date': claim_date,\n",
    "            'speaker': speaker,\n",
    "            'reporting_source': reporting_source,\n",
    "            'generated_question_1': row[\"generated_question_1\"], \n",
    "            'document_url_question_1': row[\"document_url_question_1\"], \n",
    "            'summary_1': row[\"summary_1\"], \n",
    "            'label_1': row[\"label_1\"],\n",
    "            'generated_question_2': row[\"generated_question_2\"], \n",
    "            'document_url_question_2': row[\"document_url_question_2\"], \n",
    "            'summary_2': row[\"summary_2\"], \n",
    "            'label_2': row[\"label_2\"],\n",
    "            'generated_question_3': question,\n",
    "            'document_url_question_3': best_not_enough_evidence_url, \n",
    "            'summary_3': best_not_enough_evidence_summary,\n",
    "            'label_3': 'Not Enough Evidence'\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Create an empty DataFrame to store the results\n",
    "final_df = pd.DataFrame(columns=['claim_id', 'claim', 'generated_question_1', 'document_url_question_1', 'summary_1', 'label_1', 'document_url_question_2', 'generated_question_2', 'summary_2', 'label_2', 'generated_question_3', 'summary_3', 'label_3'])\n",
    "\n",
    "# Apply the processing function to each row\n",
    "for index, row in processed_df.iterrows():\n",
    "    processed_row_df = process_row(row)\n",
    "    final_df = pd.concat([final_df, processed_row_df], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8df5e997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim_id</th>\n",
       "      <th>claim</th>\n",
       "      <th>generated_question_1</th>\n",
       "      <th>document_url_question_1</th>\n",
       "      <th>summary_1</th>\n",
       "      <th>label_1</th>\n",
       "      <th>document_url_question_2</th>\n",
       "      <th>generated_question_2</th>\n",
       "      <th>summary_2</th>\n",
       "      <th>label_2</th>\n",
       "      <th>generated_question_3</th>\n",
       "      <th>summary_3</th>\n",
       "      <th>label_3</th>\n",
       "      <th>claim_date</th>\n",
       "      <th>speaker</th>\n",
       "      <th>reporting_source</th>\n",
       "      <th>document_url_question_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Due to Imran Khan's criticism of Macron's comm...</td>\n",
       "      <td>Did French authorities cancel the visas of 183...</td>\n",
       "      <td>https://tribune.com.pk/story/1119830/diplomati...</td>\n",
       "      <td>The text contradicts the question. There is no...</td>\n",
       "      <td>Refuted</td>\n",
       "      <td>The summarized information contradicts the que...</td>\n",
       "      <td>Did French authorities deport 118 Pakistani ci...</td>\n",
       "      <td>The summarized information contradicts the que...</td>\n",
       "      <td>Refuted</td>\n",
       "      <td>Did French authorities deport 118 Pakistani ci...</td>\n",
       "      <td>Yes, Imran Khan criticized Macron's comments o...</td>\n",
       "      <td>Supported</td>\n",
       "      <td>31-10-2020</td>\n",
       "      <td>Consulate General Of Pakistan France</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>Yes, Imran Khan criticized Macron's comments o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  claim_id                                              claim  \\\n",
       "0        2  Due to Imran Khan's criticism of Macron's comm...   \n",
       "\n",
       "                                generated_question_1  \\\n",
       "0  Did French authorities cancel the visas of 183...   \n",
       "\n",
       "                             document_url_question_1  \\\n",
       "0  https://tribune.com.pk/story/1119830/diplomati...   \n",
       "\n",
       "                                           summary_1  label_1  \\\n",
       "0  The text contradicts the question. There is no...  Refuted   \n",
       "\n",
       "                             document_url_question_2  \\\n",
       "0  The summarized information contradicts the que...   \n",
       "\n",
       "                                generated_question_2  \\\n",
       "0  Did French authorities deport 118 Pakistani ci...   \n",
       "\n",
       "                                           summary_2  label_2  \\\n",
       "0  The summarized information contradicts the que...  Refuted   \n",
       "\n",
       "                                generated_question_3  \\\n",
       "0  Did French authorities deport 118 Pakistani ci...   \n",
       "\n",
       "                                           summary_3    label_3  claim_date  \\\n",
       "0  Yes, Imran Khan criticized Macron's comments o...  Supported  31-10-2020   \n",
       "\n",
       "                                speaker reporting_source  \\\n",
       "0  Consulate General Of Pakistan France          Twitter   \n",
       "\n",
       "                             document_url_question_3  \n",
       "0  Yes, Imran Khan criticized Macron's comments o...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "721a38ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#getting judgement to the summary and question based on the summary\n",
    "\n",
    "def get_judgement(claim, question_1, summary_1, question_2, summary_2, question_3, summary_3):\n",
    "    judgement_input = JUDGEMENT_PROMPT.replace('[[CLAIM]]', claim.strip())\n",
    "    judgement_input = judgement_input.replace('[[QUESTION_1]]', question_1.strip())\n",
    "    judgement_input = judgement_input.replace('[[SUMMARY_2]]', summary_1.strip())\n",
    "    judgement_input = judgement_input.replace('[[QUESTION_1]]', question_2.strip())\n",
    "    judgement_input = judgement_input.replace('[[SUMMARY_2]]', summary_2.strip())\n",
    "    judgement_input = judgement_input.replace('[[QUESTION_3]]', question_3.strip())\n",
    "    judgement_input = judgement_input.replace('[[SUMMARY_3]]', summary_3.strip())\n",
    "    \n",
    "    completion = client.chat.completions.create(\n",
    "      model=MODEL,\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert annotator who assist in providing answer based on the provided information of claim, two questions and corresponding extracted summaries of the information.\"},\n",
    "        {\"role\": \"user\", \"content\":judgement_input}\n",
    "      ]\n",
    "    )\n",
    "    \n",
    "    answer = completion.choices[0].message.content\n",
    "    return answer\n",
    "\n",
    "# Add a new column for judgements\n",
    "final_df['judgement'] = final_df.apply(lambda row: get_judgement(row['claim'], row['generated_question_1'], row['summary_1'], row['generated_question_2'], row['summary_2'], row['generated_question_3'], row['summary_3']), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "31e7d418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>generated_question_1</th>\n",
       "      <th>summary_1</th>\n",
       "      <th>generated_question_2</th>\n",
       "      <th>summary_2</th>\n",
       "      <th>generated_question_3</th>\n",
       "      <th>summary_3</th>\n",
       "      <th>judgement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Due to Imran Khan's criticism of Macron's comm...</td>\n",
       "      <td>Did French authorities cancel the visas of 183...</td>\n",
       "      <td>The text contradicts the question. There is no...</td>\n",
       "      <td>Did French authorities deport 118 Pakistani ci...</td>\n",
       "      <td>The summarized information contradicts the que...</td>\n",
       "      <td>Did French authorities deport 118 Pakistani ci...</td>\n",
       "      <td>Yes, Imran Khan criticized Macron's comments o...</td>\n",
       "      <td>Based on the provided summaries, it can be con...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim  \\\n",
       "0  Due to Imran Khan's criticism of Macron's comm...   \n",
       "\n",
       "                                generated_question_1  \\\n",
       "0  Did French authorities cancel the visas of 183...   \n",
       "\n",
       "                                           summary_1  \\\n",
       "0  The text contradicts the question. There is no...   \n",
       "\n",
       "                                generated_question_2  \\\n",
       "0  Did French authorities deport 118 Pakistani ci...   \n",
       "\n",
       "                                           summary_2  \\\n",
       "0  The summarized information contradicts the que...   \n",
       "\n",
       "                                generated_question_3  \\\n",
       "0  Did French authorities deport 118 Pakistani ci...   \n",
       "\n",
       "                                           summary_3  \\\n",
       "0  Yes, Imran Khan criticized Macron's comments o...   \n",
       "\n",
       "                                           judgement  \n",
       "0  Based on the provided summaries, it can be con...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df[[\"claim\", \"generated_question_1\", \"summary_1\", \"generated_question_2\", \"summary_2\",  \"generated_question_3\", \"summary_3\", \"judgement\"]].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "45059fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_rows = []\n",
    "\n",
    "for index, row in final_df.iterrows():\n",
    "    remaining_row = {\n",
    "        'claim_id': row['claim_id'],\n",
    "        'claim': row['claim'],\n",
    "        'claim_date': row['claim_date'],\n",
    "        'speaker': row['speaker'],\n",
    "        'reporting_source': row['reporting_source'],\n",
    "        'generated_question_1': row['generated_question_1'],\n",
    "        'summary_1': row['summary_1'],\n",
    "        'label_1': row['label_1'],\n",
    "        'document_url_question_1': row['document_url_question_1'],\n",
    "        'generated_question_2': row['generated_question_2'],\n",
    "        'summary_2': row['summary_2'],\n",
    "        'label_2': row['label_2'],\n",
    "        'document_url_question_2': row['document_url_question_2'],\n",
    "        'generated_question_3': row['generated_question_3'],\n",
    "        'summary_3': row['summary_3'],\n",
    "        'label_3': row['label_3'],\n",
    "        'document_url_question_3': row['document_url_question_3'],\n",
    "        'judgement': row['judgement'].replace(\"\\n\", \" \").strip(),\n",
    "    }\n",
    "        \n",
    "    remaining_rows.append(remaining_row)\n",
    "remaining_df = pd.DataFrame(remaining_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cf3fedc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_df.to_csv('../final/out_csvs/q3_enough_generated.csv', index  = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "90298b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_df.drop('label_2', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "b38cec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, row in processed_df.iterrows():\n",
    "#     row['label_1'] = row['label_1'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "4d85338a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "86a0ea5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
